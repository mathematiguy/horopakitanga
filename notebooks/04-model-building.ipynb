{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e233e7-efbe-40ea-b94e-fcf689d677f6",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n",
    "Now we start designing models to do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1f178-b460-4773-9591-bfbd2e7975c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ataarangi.train import encode_world_state, TextTokenizer, WorldStateTokenizer, RākauDataset\n",
    "\n",
    "# Initialize tokenizers\n",
    "world_state_tokenizer = WorldStateTokenizer()\n",
    "text_tokenizer = TextTokenizer()\n",
    "\n",
    "rākau_data = pd.read_csv('../data/rākau_data.csv')\n",
    "rākau_data['rākau'] = rākau_data.rākau.apply(json.loads)\n",
    "rākau_data = rākau_data[rākau_data.num_rākau <= 10].reset_index(drop=True)\n",
    "\n",
    "rākau_data.sort_values('num_rākau', ascending=False)\n",
    "\n",
    "text_tokenizer = TextTokenizer()\n",
    "ws_tokenizer = WorldStateTokenizer()\n",
    "\n",
    "rākau_data['input'] = rākau_data.rākau.apply(ws_tokenizer.tokenize)\n",
    "rākau_data['target'] = rākau_data.description.apply(text_tokenizer.tokenize)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Create a positional encoding that is large enough for any sequence you expect to process\n",
    "        self.register_buffer('positional_encodings', self.create_positional_encodings(max_seq_length, embed_size))\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def create_positional_encodings(self, max_len, embed_size):\n",
    "        \"\"\"Create positional encodings for transformer model.\"\"\"\n",
    "        pos_enc = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc.unsqueeze(0)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_pos = self.positional_encodings[:, :src.size(1), :]\n",
    "        tgt_pos = self.positional_encodings[:, :tgt.size(1), :]\n",
    "        src = self.embedding(src) + src_pos\n",
    "        tgt = self.embedding(tgt) + tgt_pos\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Model instantiation\n",
    "model = TransformerModel(\n",
    "    vocab_size=max(text_tokenizer.token_map.values())+1,\n",
    "    embed_size=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    max_seq_length=500,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extracting input_ids, token_type_ids, and attention_mask from the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "\n",
    "    # Find the maximum sequence length in this batch\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "\n",
    "    # Pad all sequences to this maximum length\n",
    "    padded_input_ids = torch.stack([torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)]) for ids in input_ids])\n",
    "    padded_token_type_ids = torch.stack([torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)]) for ids in token_type_ids])\n",
    "    padded_attention_mask = torch.stack([torch.cat([mask, torch.zeros(max_len - len(mask), dtype=torch.long)]) for mask in attention_mask])\n",
    "\n",
    "    return {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'token_type_ids': padded_token_type_ids,\n",
    "        'attention_mask': padded_attention_mask\n",
    "    }\n",
    "\n",
    "# Create dataset\n",
    "dataset = RākauDataset(rākau_data.rākau, rākau_data.description, world_state_tokenizer, text_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Assuming 'input_ids' are the source and target sequences\n",
    "        src = batch['input_ids'][:, :-1].to(device)  # all but the last for input\n",
    "        tgt = batch['input_ids'][:, 1:].to(device)   # all but the first for target\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        # Compute loss; assume output is reshaped to (batch_size*seq_len, vocab_size)\n",
    "        # and tgt is reshaped accordingly for CrossEntropyLoss\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt.reshape(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optional: Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    print(f'Epoch {epoch+1} completed, Average Loss: {epoch_loss / len(dataloader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
