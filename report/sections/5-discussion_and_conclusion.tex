The labelling task was designed to represent the kind of task that might be expected of a beginner language learner, and the dataset was constructed to have many ($\sim$50+) times the amount of data that would ordinarily be needed by a human to learn this sort of task. Because of this, it was surprising that a model was not able to effectively learn the goal task, even with the significant data augmentation and heavy guidance provided by the 1-gram model.

It is possible the problem was still too complex for a simple model to solve, and we could have tried removing some variables such as length, or reducing the size of some of the categories. This might have been one way to verify that the task was solvable in principle.

Another thing that might have helped is to train the model for far longer. The longer training runs we ran took around six hours to run to completion and over 1,000 epochs. While the loss for some runs was below 0.000001, we still found the model had not learned how to produce a plausible looking sequence.

The overall aim of the project was to design and test a learning system that functioned in a similar way to the Te Ataarangi method. Some progress was made towards this goal in terms of formulating the problem, but having a competent model at the target task is necessary for that model to act as a teacher and test out the theory further.

At the same time, the task we designed has some potential as a simple world model that can use used to test low-resource languages. It can be simplified further, or extended in new ways, similar to a grid-world. In the classroom the rods are used in many other ways in practice. It seems possible that we got close to training a competent model, and if we had achieved that it would have been interesting to investigate the other parts of the problem further.
